{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, KFold\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge, Lasso, ElasticNet\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from tabulate import tabulate\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Function to create JSON-serializable dictionaries\n",
        "def make_serializable(obj):\n",
        "    if isinstance(obj, np.ndarray):\n",
        "        return obj.tolist()\n",
        "    elif isinstance(obj, pd.DataFrame):\n",
        "        return obj.to_dict(orient='records')\n",
        "    else:\n",
        "        return str(obj)\n",
        "\n",
        "# Function to display table from results data\n",
        "def display_results_table(results, table_name, headers):\n",
        "    print(f\"\\n{table_name} Results:\")\n",
        "    table_data = []\n",
        "    for result in results:\n",
        "        row = [result[key] for key in headers.keys()]\n",
        "        table_data.append(row)\n",
        "    print(tabulate(table_data, headers=headers.values(), tablefmt=\"pretty\", floatfmt=\".4f\"))\n",
        "\n",
        "# Function to display feature importance from dictionary\n",
        "def display_feature_importance(feature_importance_dict, title):\n",
        "    print(f\"\\n{title} Feature Importance:\")\n",
        "    for model_name, importance_data in feature_importance_dict.items():\n",
        "        df = pd.DataFrame(importance_data)\n",
        "        print(f\"\\nTop 5 Features for {model_name}:\")\n",
        "        print(tabulate(df.head(5), headers='keys', tablefmt=\"pretty\", floatfmt=\".4f\"))\n",
        "\n",
        "# Function to display hyperparameters\n",
        "def display_hyperparameters(hyperparams_dict, title):\n",
        "    print(f\"\\n{title} Best Hyperparameters:\")\n",
        "    for model_name, params in hyperparams_dict.items():\n",
        "        print(f\"\\n{model_name}:\")\n",
        "        if isinstance(params, str):\n",
        "            print(params)\n",
        "        else:\n",
        "            # Convert to a table format\n",
        "            param_table = [[k, v] for k, v in params.items()]\n",
        "            print(tabulate(param_table, headers=['Parameter', 'Value'], tablefmt=\"pretty\"))\n",
        "\n",
        "# Function to display confusion matrix\n",
        "def display_confusion_matrix(cm):\n",
        "    df_cm = pd.DataFrame(\n",
        "        cm,\n",
        "        index=['Actual 0', 'Actual 1'],\n",
        "        columns=['Predicted 0', 'Predicted 1']\n",
        "    )\n",
        "    return tabulate(df_cm, headers='keys', tablefmt=\"pretty\")\n",
        "\n",
        "# Function to display PCA parameters\n",
        "def display_pca_parameters(pca_params):\n",
        "    print(\"\\nPCA Parameters:\")\n",
        "    param_table = [\n",
        "        [\"Number of Components\", pca_params[\"n_components\"]],\n",
        "        [\"Explained Variance\", f\"{sum(pca_params['explained_variance_ratio'])*100:.2f}%\"]\n",
        "    ]\n",
        "    print(tabulate(param_table, headers=['Parameter', 'Value'], tablefmt=\"pretty\"))\n",
        "\n",
        "# Function to create confusion matrix and get classification metrics\n",
        "def get_classification_metrics(y_true, y_pred):\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred, average='weighted')\n",
        "    recall = recall_score(y_true, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    return accuracy, precision, recall, f1, cm\n",
        "\n",
        "# Function to get regression metrics\n",
        "def get_regression_metrics(y_true, y_pred):\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    return rmse, mae, r2\n",
        "\n",
        "# Function to calculate feature importance\n",
        "def get_feature_importance(model, feature_names, pca_components=None):\n",
        "    \"\"\"\n",
        "    Calculate feature importance, including support for PCA transformed data\n",
        "    \"\"\"\n",
        "    if pca_components is not None:\n",
        "        # For PCA-transformed data, we need to map back to original features\n",
        "        if hasattr(model, 'feature_importances_'):\n",
        "            pca_importance = model.feature_importances_\n",
        "            importance = np.abs(np.dot(pca_importance, pca_components))\n",
        "        elif hasattr(model, 'coef_'):\n",
        "            pca_importance = np.abs(model.coef_).mean(axis=0) if model.coef_.ndim > 1 else np.abs(model.coef_)\n",
        "            importance = np.abs(np.dot(pca_importance, pca_components))\n",
        "        else:\n",
        "            return None\n",
        "    elif hasattr(model, 'feature_importances_'):\n",
        "        importance = model.feature_importances_\n",
        "    elif hasattr(model, 'coef_'):\n",
        "        importance = np.abs(model.coef_).mean(axis=0) if model.coef_.ndim > 1 else np.abs(model.coef_)\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'Feature': feature_names,\n",
        "        'Importance': importance\n",
        "    })\n",
        "    return feature_importance.sort_values('Importance', ascending=False)\n",
        "\n",
        "# Main function to run all experiments and save progress\n",
        "def run_experiments(loan_data_path, real_estate_path, sample_fraction=0.3, random_state=42):\n",
        "    progress = {\n",
        "        \"Table_1\": {\"Results\": [], \"Feature_Importance\": {}},\n",
        "        \"Table_2\": {\"Results\": []},\n",
        "        \"Table_3\": {\"Results\": [], \"Best_Hyperparameters\": {}, \"Feature_Importance\": {}},\n",
        "        \"Table_4\": {\"Results\": [], \"Feature_Importance\": {}, \"PCA_Parameters\": {}},\n",
        "        \"Regression_Table_1\": {\"Results\": [], \"Feature_Importance\": {}},\n",
        "        \"Regression_Table_2\": {\"Results\": [], \"Best_Hyperparameters\": {}, \"Feature_Importance\": {}}\n",
        "    }\n",
        "\n",
        "    # Load and preprocess loan data for classification\n",
        "    print(\"Loading and preprocessing loan data for classification...\")\n",
        "    loan_data = pd.read_csv(loan_data_path)\n",
        "    loan_data = loan_data.dropna()\n",
        "\n",
        "    # Sample fraction of the dataset\n",
        "    loan_data = loan_data.sample(frac=sample_fraction, random_state=random_state)\n",
        "    print(f\"Using {len(loan_data)} samples ({sample_fraction*100}% of original dataset)\")\n",
        "\n",
        "    # Convert categorical features to numerical\n",
        "    cat_cols = ['person_gender', 'person_education', 'person_home_ownership',\n",
        "                'loan_intent', 'previous_loan_defaults_on_file']\n",
        "    for col in cat_cols:\n",
        "        if col in loan_data.columns:\n",
        "            loan_data[col] = loan_data[col].astype('category').cat.codes\n",
        "\n",
        "    # Prepare data for modeling\n",
        "    X = loan_data.drop(['loan_status'], axis=1)\n",
        "    y = loan_data['loan_status']\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
        "    print(f\"Train set: {X_train.shape[0]} samples, Test set: {X_test.shape[0]} samples\")\n",
        "\n",
        "    # Define classifiers\n",
        "    classifiers = {\n",
        "        'Logistic Regression': LogisticRegression(max_iter=1000, random_state=random_state),\n",
        "        'Decision Tree': DecisionTreeClassifier(random_state=random_state),\n",
        "        'Random Forest': RandomForestClassifier(random_state=random_state),\n",
        "        'Gradient Boosting': GradientBoostingClassifier(random_state=random_state),\n",
        "        'K-Nearest Neighbors': KNeighborsClassifier(),\n",
        "        'Naive Bayes': GaussianNB()\n",
        "    }\n",
        "\n",
        "    # TABLE 1: Base classifiers without scaling\n",
        "    print(\"\\n--- TABLE 1: Base classifiers without scaling ---\")\n",
        "    for name in tqdm(classifiers.keys(), desc=\"Training base classifiers\"):\n",
        "        classifier = classifiers[name]\n",
        "        start_time = time.time()\n",
        "        classifier.fit(X_train, y_train)\n",
        "        y_pred = classifier.predict(X_test)\n",
        "        train_time = time.time() - start_time\n",
        "        accuracy, precision, recall, f1, cm = get_classification_metrics(y_test, y_pred)\n",
        "\n",
        "        # Store results\n",
        "        result = {\n",
        "            \"Classifier\": name,\n",
        "            \"Accuracy\": float(accuracy),\n",
        "            \"Precision\": float(precision),\n",
        "            \"Recall\": float(recall),\n",
        "            \"F1-Score\": float(f1),\n",
        "            \"Time(s)\": float(train_time),\n",
        "            \"Confusion_Matrix\": cm.tolist()\n",
        "        }\n",
        "        progress[\"Table_1\"][\"Results\"].append(result)\n",
        "\n",
        "        # Calculate feature importance if available\n",
        "        if name not in ['K-Nearest Neighbors', 'Naive Bayes']:\n",
        "            feature_imp = get_feature_importance(classifier, X.columns)\n",
        "            if feature_imp is not None:\n",
        "                progress[\"Table_1\"][\"Feature_Importance\"][name] = feature_imp.to_dict(orient='records')\n",
        "\n",
        "        # Save progress after each model\n",
        "        with open(\"progress.json\", \"w\") as json_file:\n",
        "            json.dump(progress, json_file, indent=4, default=make_serializable)\n",
        "\n",
        "    # Display Table 1 results in console\n",
        "    display_results_table(\n",
        "        progress[\"Table_1\"][\"Results\"],\n",
        "        \"TABLE 1: Base classifiers without scaling\",\n",
        "        {\"Classifier\": \"Classifier\", \"Accuracy\": \"Accuracy\", \"Precision\": \"Precision\",\n",
        "         \"Recall\": \"Recall\", \"F1-Score\": \"F1-Score\", \"Time(s)\": \"Time (s)\"}\n",
        "    )\n",
        "\n",
        "    # Display feature importance\n",
        "    display_feature_importance(progress[\"Table_1\"][\"Feature_Importance\"], \"TABLE 1\")\n",
        "\n",
        "    # Display confusion matrices\n",
        "    print(\"\\nTABLE 1: Confusion Matrices:\")\n",
        "    for result in progress[\"Table_1\"][\"Results\"]:\n",
        "        print(f\"\\nConfusion Matrix for {result['Classifier']}:\")\n",
        "        print(display_confusion_matrix(result[\"Confusion_Matrix\"]))\n",
        "\n",
        "    # TABLE 2: Testing different scaling methods\n",
        "    print(\"\\n--- TABLE 2: Testing different scaling methods ---\")\n",
        "    scalers = {\n",
        "        'L1 Normalization': Normalizer(norm='l1'),\n",
        "        'L2 Normalization': Normalizer(norm='l2'),\n",
        "        'Min-Max Scaling': MinMaxScaler(),\n",
        "        'Std Scaling': StandardScaler()\n",
        "    }\n",
        "\n",
        "    best_scaler = None\n",
        "    best_scaler_name = None\n",
        "    best_accuracy = 0\n",
        "\n",
        "    for scaler_name in tqdm(scalers.keys(), desc=\"Testing scaling methods\"):\n",
        "        scaler = scalers[scaler_name]\n",
        "        X_train_scaled = scaler.fit_transform(X_train)\n",
        "        X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "        for classifier_name in tqdm(classifiers.keys(), desc=f\"Training with {scaler_name}\", leave=False):\n",
        "            classifier = classifiers[classifier_name]\n",
        "            classifier.fit(X_train_scaled, y_train)\n",
        "            y_pred = classifier.predict(X_test_scaled)\n",
        "            accuracy, precision, recall, f1, cm = get_classification_metrics(y_test, y_pred)\n",
        "\n",
        "            # Store results\n",
        "            result = {\n",
        "                \"Scaling_Method\": scaler_name,\n",
        "                \"Classifier\": classifier_name,\n",
        "                \"Accuracy\": float(accuracy),\n",
        "                \"Precision\": float(precision),\n",
        "                \"Recall\": float(recall),\n",
        "                \"F1-Score\": float(f1),\n",
        "                \"Confusion_Matrix\": cm.tolist()\n",
        "            }\n",
        "            progress[\"Table_2\"][\"Results\"].append(result)\n",
        "\n",
        "            # Track best scaler performance\n",
        "            if accuracy > best_accuracy:\n",
        "                best_accuracy = accuracy\n",
        "                best_scaler = scaler\n",
        "                best_scaler_name = scaler_name\n",
        "\n",
        "        # Save progress after each scaler\n",
        "        with open(\"progress.json\", \"w\") as json_file:\n",
        "            json.dump(progress, json_file, indent=4, default=make_serializable)\n",
        "\n",
        "    # Display Table 2 results in console\n",
        "    display_results_table(\n",
        "        progress[\"Table_2\"][\"Results\"],\n",
        "        \"TABLE 2: Scaling methods comparison\",\n",
        "        {\"Scaling_Method\": \"Scaling Method\", \"Classifier\": \"Classifier\", \"Accuracy\": \"Accuracy\",\n",
        "         \"Precision\": \"Precision\", \"Recall\": \"Recall\", \"F1-Score\": \"F1-Score\"}\n",
        "    )\n",
        "\n",
        "    print(f\"\\nBest Scaling Method: {best_scaler_name} (Accuracy: {best_accuracy:.4f})\")\n",
        "\n",
        "    # Apply best scaler for future use\n",
        "    print(f\"\\nApplying best scaler ({best_scaler_name}) to the data...\")\n",
        "    X_train_best_scaled = best_scaler.fit_transform(X_train)\n",
        "    X_test_best_scaled = best_scaler.transform(X_test)\n",
        "\n",
        "    # TABLE 3: Grid search with 10-fold CV using best scaler\n",
        "    print(\"\\n--- TABLE 3: Grid search with 10-fold CV using best scaler ---\")\n",
        "    param_grids = {\n",
        "        'Logistic Regression': {\n",
        "            'classifier__C': [0.01, 0.1, 1, 10, 100],\n",
        "            'classifier__solver': ['liblinear', 'saga']\n",
        "        },\n",
        "        'Decision Tree': {\n",
        "            'classifier__max_depth': [None, 10, 20, 30],\n",
        "            'classifier__min_samples_split': [2, 5, 10]\n",
        "        },\n",
        "        'Random Forest': {\n",
        "            'classifier__n_estimators': [50, 100, 200],\n",
        "            'classifier__max_depth': [None, 10, 20]\n",
        "        },\n",
        "        'Gradient Boosting': {\n",
        "            'classifier__n_estimators': [50, 100, 200],\n",
        "            'classifier__learning_rate': [0.01, 0.1, 0.2]\n",
        "        },\n",
        "        'K-Nearest Neighbors': {\n",
        "            'classifier__n_neighbors': [3, 5, 7, 9],\n",
        "            'classifier__weights': ['uniform', 'distance']\n",
        "        },\n",
        "        'Naive Bayes': {}  # No hyperparameters to tune\n",
        "    }\n",
        "\n",
        "    best_models_table3 = {}\n",
        "    kf = KFold(n_splits=10, shuffle=True, random_state=random_state)\n",
        "\n",
        "    for name in tqdm(classifiers.keys(), desc=\"Performing grid search with CV\"):\n",
        "        classifier = classifiers[name]\n",
        "        # Create pipeline with best scaler\n",
        "        pipeline = Pipeline([\n",
        "            ('scaler', best_scaler.__class__()),  # Use same scaler type\n",
        "            ('classifier', classifier)\n",
        "        ])\n",
        "\n",
        "        if name == 'Naive Bayes':\n",
        "            # For Naive Bayes, just do cross-validation\n",
        "            cv_scores = cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='accuracy')\n",
        "            pipeline.fit(X_train, y_train)\n",
        "            y_pred = pipeline.predict(X_test)\n",
        "            accuracy, precision, recall, f1, cm = get_classification_metrics(y_test, y_pred)\n",
        "\n",
        "            # Store results\n",
        "            result = {\n",
        "                \"Classifier\": name,\n",
        "                \"CV_Accuracy\": float(np.mean(cv_scores)),\n",
        "                \"Test_Accuracy\": float(accuracy),\n",
        "                \"Precision\": float(precision),\n",
        "                \"Recall\": float(recall),\n",
        "                \"F1-Score\": float(f1),\n",
        "                \"Confusion_Matrix\": cm.tolist()\n",
        "            }\n",
        "            progress[\"Table_3\"][\"Results\"].append(result)\n",
        "            progress[\"Table_3\"][\"Best_Hyperparameters\"][name] = \"No hyperparameters to tune\"\n",
        "            best_models_table3[name] = pipeline\n",
        "        else:\n",
        "            # Grid search with CV for other classifiers\n",
        "            grid_search = GridSearchCV(\n",
        "                pipeline,\n",
        "                param_grids[name],\n",
        "                cv=kf,\n",
        "                scoring='accuracy',\n",
        "                n_jobs=-1\n",
        "            )\n",
        "            grid_search.fit(X_train, y_train)\n",
        "            y_pred = grid_search.predict(X_test)\n",
        "            accuracy, precision, recall, f1, cm = get_classification_metrics(y_test, y_pred)\n",
        "\n",
        "            # Store results\n",
        "            result = {\n",
        "                \"Classifier\": name,\n",
        "                \"CV_Accuracy\": float(grid_search.best_score_),\n",
        "                \"Test_Accuracy\": float(accuracy),\n",
        "                \"Precision\": float(precision),\n",
        "                \"Recall\": float(recall),\n",
        "                \"F1-Score\": float(f1),\n",
        "                \"Confusion_Matrix\": cm.tolist()\n",
        "            }\n",
        "            progress[\"Table_3\"][\"Results\"].append(result)\n",
        "            progress[\"Table_3\"][\"Best_Hyperparameters\"][name] = grid_search.best_params_\n",
        "            best_models_table3[name] = grid_search.best_estimator_\n",
        "\n",
        "            # Calculate feature importance for the best model\n",
        "            if name not in ['K-Nearest Neighbors', 'Naive Bayes']:\n",
        "                try:\n",
        "                    feature_imp = get_feature_importance(grid_search.best_estimator_.named_steps['classifier'], X.columns)\n",
        "                    if feature_imp is not None:\n",
        "                        progress[\"Table_3\"][\"Feature_Importance\"][name] = feature_imp.to_dict(orient='records')\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "        # Save progress after each model\n",
        "        with open(\"progress.json\", \"w\") as json_file:\n",
        "            json.dump(progress, json_file, indent=4, default=make_serializable)\n",
        "\n",
        "    # Display Table 3 results in console\n",
        "    display_results_table(\n",
        "        progress[\"Table_3\"][\"Results\"],\n",
        "        \"TABLE 3: Grid search with 10-fold CV\",\n",
        "        {\"Classifier\": \"Classifier\", \"CV_Accuracy\": \"CV Accuracy\", \"Test_Accuracy\": \"Test Accuracy\",\n",
        "         \"Precision\": \"Precision\", \"Recall\": \"Recall\", \"F1-Score\": \"F1-Score\"}\n",
        "    )\n",
        "\n",
        "    # Display best hyperparameters\n",
        "    display_hyperparameters(progress[\"Table_3\"][\"Best_Hyperparameters\"], \"TABLE 3\")\n",
        "\n",
        "    # Display feature importance\n",
        "    display_feature_importance(progress[\"Table_3\"][\"Feature_Importance\"], \"TABLE 3\")\n",
        "\n",
        "    # Display confusion matrices\n",
        "    print(\"\\nTABLE 3: Confusion Matrices:\")\n",
        "    for result in progress[\"Table_3\"][\"Results\"]:\n",
        "        print(f\"\\nConfusion Matrix for {result['Classifier']}:\")\n",
        "        print(display_confusion_matrix(result[\"Confusion_Matrix\"]))\n",
        "\n",
        "    # TABLE 4: PCA with best scaling method\n",
        "    print(\"\\n--- TABLE 4: PCA with best scaling method ---\")\n",
        "    pca = PCA(n_components=0.95)  # Keep 95% of variance\n",
        "    X_train_pca = pca.fit_transform(X_train_best_scaled)\n",
        "    X_test_pca = pca.transform(X_test_best_scaled)\n",
        "    print(f\"Reduced from {X_train.shape[1]} to {X_train_pca.shape[1]} features\")\n",
        "\n",
        "    # Store PCA parameters\n",
        "    progress[\"Table_4\"][\"PCA_Parameters\"] = {\n",
        "        \"n_components\": int(pca.n_components_),\n",
        "        \"explained_variance_ratio\": pca.explained_variance_ratio_.tolist(),\n",
        "        \"singular_values\": pca.singular_values_.tolist()\n",
        "    }\n",
        "\n",
        "    # Display PCA parameters\n",
        "    display_pca_parameters(progress[\"Table_4\"][\"PCA_Parameters\"])\n",
        "\n",
        "    for name in tqdm(classifiers.keys(), desc=\"Training with PCA-transformed data\"):\n",
        "        # Use best parameters from Table 3\n",
        "        if name == 'Naive Bayes':\n",
        "            model = GaussianNB()\n",
        "        elif name == 'Logistic Regression':\n",
        "            best_params = progress[\"Table_3\"][\"Best_Hyperparameters\"][name]\n",
        "            classifier_params = {k.replace('classifier__', ''): v for k, v in best_params.items() if 'classifier__' in k}\n",
        "            model = LogisticRegression(**classifier_params, max_iter=1000, random_state=random_state)\n",
        "        elif name == 'Decision Tree':\n",
        "            best_params = progress[\"Table_3\"][\"Best_Hyperparameters\"][name]\n",
        "            classifier_params = {k.replace('classifier__', ''): v for k, v in best_params.items() if 'classifier__' in k}\n",
        "            model = DecisionTreeClassifier(**classifier_params, random_state=random_state)\n",
        "        elif name == 'Random Forest':\n",
        "            best_params = progress[\"Table_3\"][\"Best_Hyperparameters\"][name]\n",
        "            classifier_params = {k.replace('classifier__', ''): v for k, v in best_params.items() if 'classifier__' in k}\n",
        "            model = RandomForestClassifier(**classifier_params, random_state=random_state)\n",
        "        elif name == 'Gradient Boosting':\n",
        "            best_params = progress[\"Table_3\"][\"Best_Hyperparameters\"][name]\n",
        "            classifier_params = {k.replace('classifier__', ''): v for k, v in best_params.items() if 'classifier__' in k}\n",
        "            model = GradientBoostingClassifier(**classifier_params, random_state=random_state)\n",
        "        elif name == 'K-Nearest Neighbors':\n",
        "            best_params = progress[\"Table_3\"][\"Best_Hyperparameters\"][name]\n",
        "            classifier_params = {k.replace('classifier__', ''): v for k, v in best_params.items() if 'classifier__' in k}\n",
        "            model = KNeighborsClassifier(**classifier_params)\n",
        "\n",
        "        model.fit(X_train_pca, y_train)\n",
        "        y_pred = model.predict(X_test_pca)\n",
        "        accuracy, precision, recall, f1, cm = get_classification_metrics(y_test, y_pred)\n",
        "\n",
        "        # Store results\n",
        "        result = {\n",
        "            \"Classifier\": name,\n",
        "            \"Accuracy\": float(accuracy),\n",
        "            \"Precision\": float(precision),\n",
        "            \"Recall\": float(recall),\n",
        "            \"F1-Score\": float(f1),\n",
        "            \"Confusion_Matrix\": cm.tolist()\n",
        "        }\n",
        "        progress[\"Table_4\"][\"Results\"].append(result)\n",
        "\n",
        "        # Calculate feature importance for PCA models\n",
        "        if name not in ['K-Nearest Neighbors', 'Naive Bayes']:\n",
        "            try:\n",
        "                # For PCA, we need to map back to original features\n",
        "                feature_imp = get_feature_importance(model, X.columns, pca.components_)\n",
        "                if feature_imp is not None:\n",
        "                    progress[\"Table_4\"][\"Feature_Importance\"][name] = feature_imp.to_dict(orient='records')\n",
        "            except Exception as e:\n",
        "                print(f\"Error calculating feature importance for {name}: {e}\")\n",
        "\n",
        "        # Save progress after each model\n",
        "        with open(\"progress.json\", \"w\") as json_file:\n",
        "            json.dump(progress, json_file, indent=4, default=make_serializable)\n",
        "\n",
        "    # Display Table 4 results in console\n",
        "    display_results_table(\n",
        "        progress[\"Table_4\"][\"Results\"],\n",
        "        \"TABLE 4: PCA with best scaling method\",\n",
        "        {\"Classifier\": \"Classifier\", \"Accuracy\": \"Accuracy\", \"Precision\": \"Precision\",\n",
        "         \"Recall\": \"Recall\", \"F1-Score\": \"F1-Score\"}\n",
        "    )\n",
        "\n",
        "    # Display feature importance\n",
        "    display_feature_importance(progress[\"Table_4\"][\"Feature_Importance\"], \"TABLE 4\")\n",
        "\n",
        "    # Display confusion matrices\n",
        "    print(\"\\nTABLE 4: Confusion Matrices:\")\n",
        "    for result in progress[\"Table_4\"][\"Results\"]:\n",
        "        print(f\"\\nConfusion Matrix for {result['Classifier']}:\")\n",
        "        print(display_confusion_matrix(result[\"Confusion_Matrix\"]))\n",
        "\n",
        "    # REGRESSION ANALYSIS\n",
        "    print(\"\\n\\n======== REGRESSION ANALYSIS ========\\n\")\n",
        "    print(\"Loading and preprocessing real estate data for regression...\")\n",
        "    real_estate = pd.read_csv(real_estate_path)\n",
        "    real_estate = real_estate.dropna()\n",
        "\n",
        "    # Sample fraction of the dataset\n",
        "    real_estate = real_estate.sample(frac=sample_fraction, random_state=random_state)\n",
        "    print(f\"Using {len(real_estate)} samples ({sample_fraction*100}% of original dataset)\")\n",
        "\n",
        "    # Prepare data for modeling\n",
        "    X_reg = real_estate.drop(['No', 'Y house price of unit area'], axis=1)\n",
        "    y_reg = real_estate['Y house price of unit area']\n",
        "    X_reg_train, X_reg_test, y_reg_train, y_reg_test = train_test_split(X_reg, y_reg, test_size=0.2, random_state=random_state)\n",
        "    print(f\"Train set: {X_reg_train.shape[0]} samples, Test set: {X_reg_test.shape[0]} samples\")\n",
        "\n",
        "    # Define regressors\n",
        "    regressors = {\n",
        "        'Linear Regression': LinearRegression(),\n",
        "        'Ridge Regression': Ridge(random_state=random_state),\n",
        "        'Lasso Regression': Lasso(random_state=random_state),\n",
        "        'ElasticNet': ElasticNet(random_state=random_state),\n",
        "        'Decision Tree': DecisionTreeRegressor(random_state=random_state),\n",
        "        'Random Forest': RandomForestRegressor(random_state=random_state)\n",
        "    }\n",
        "\n",
        "    # REGRESSION TABLE 1: Base regressors\n",
        "    print(\"\\n--- REGRESSION TABLE 1: Base regressors ---\")\n",
        "    for name in tqdm(regressors.keys(), desc=\"Training base regressors\"):\n",
        "        regressor = regressors[name]\n",
        "        start_time = time.time()\n",
        "        regressor.fit(X_reg_train, y_reg_train)\n",
        "        y_reg_pred = regressor.predict(X_reg_test)\n",
        "        train_time = time.time() - start_time\n",
        "        rmse, mae, r2 = get_regression_metrics(y_reg_test, y_reg_pred)\n",
        "\n",
        "        # Store results\n",
        "        result = {\n",
        "            \"Regressor\": name,\n",
        "            \"RMSE\": float(rmse),\n",
        "            \"MAE\": float(mae),\n",
        "            \"R²\": float(r2),\n",
        "            \"Time(s)\": float(train_time)\n",
        "        }\n",
        "        progress[\"Regression_Table_1\"][\"Results\"].append(result)\n",
        "\n",
        "        # Calculate feature importance if available\n",
        "        feature_imp = get_feature_importance(regressor, X_reg.columns)\n",
        "        if feature_imp is not None:\n",
        "            progress[\"Regression_Table_1\"][\"Feature_Importance\"][name] = feature_imp.to_dict(orient='records')\n",
        "\n",
        "        # Save progress after each model\n",
        "        with open(\"progress.json\", \"w\") as json_file:\n",
        "            json.dump(progress, json_file, indent=4, default=make_serializable)\n",
        "\n",
        "    # Display Regression Table 1 results in console\n",
        "    display_results_table(\n",
        "        progress[\"Regression_Table_1\"][\"Results\"],\n",
        "        \"REGRESSION TABLE 1: Base regressors\",\n",
        "        {\"Regressor\": \"Regressor\", \"RMSE\": \"RMSE\", \"MAE\": \"MAE\", \"R²\": \"R²\", \"Time(s)\": \"Time (s)\"}\n",
        "    )\n",
        "\n",
        "    # Display feature importance\n",
        "    display_feature_importance(progress[\"Regression_Table_1\"][\"Feature_Importance\"], \"REGRESSION TABLE 1\")\n",
        "\n",
        "    # REGRESSION TABLE 2: Grid search with 10-fold CV\n",
        "    print(\"\\n--- REGRESSION TABLE 2: Grid search with 10-fold CV ---\")\n",
        "    reg_param_grids = {\n",
        "        'Linear Regression': {},  # No hyperparameters to tune\n",
        "        'Ridge Regression': {\n",
        "            'regressor__alpha': [0.01, 0.1, 1, 10, 100]\n",
        "        },\n",
        "        'Lasso Regression': {\n",
        "            'regressor__alpha': [0.01, 0.1, 1, 10, 100]\n",
        "        },\n",
        "        'ElasticNet': {\n",
        "            'regressor__alpha': [0.01, 0.1, 1, 10, 100],\n",
        "            'regressor__l1_ratio': [0.1, 0.5, 0.9]\n",
        "        },\n",
        "        'Decision Tree': {\n",
        "            'regressor__max_depth': [None, 10, 20, 30],\n",
        "            'regressor__min_samples_split': [2, 5, 10]\n",
        "        },\n",
        "        'Random Forest': {\n",
        "            'regressor__n_estimators': [50, 100, 200],\n",
        "            'regressor__max_depth': [None, 10, 20]\n",
        "        }\n",
        "    }\n",
        "\n",
        "    kf = KFold(n_splits=10, shuffle=True, random_state=random_state)\n",
        "\n",
        "    for name in tqdm(regressors.keys(), desc=\"Performing grid search for regressors\"):\n",
        "        regressor = regressors[name]\n",
        "        pipeline = Pipeline([('regressor', regressor)])\n",
        "\n",
        "        if name == 'Linear Regression':\n",
        "            # Linear Regression doesn't have hyperparameters to tune\n",
        "            cv_scores = cross_val_score(pipeline, X_reg_train, y_reg_train, cv=kf,\n",
        "                                      scoring='neg_mean_squared_error')\n",
        "            pipeline.fit(X_reg_train, y_reg_train)\n",
        "            y_reg_pred = pipeline.predict(X_reg_test)\n",
        "            rmse, mae, r2 = get_regression_metrics(y_reg_test, y_reg_pred)\n",
        "\n",
        "            # Store results\n",
        "            result = {\n",
        "                \"Regressor\": name,\n",
        "                \"CV_RMSE\": float(np.sqrt(-np.mean(cv_scores))),\n",
        "                \"Test_RMSE\": float(rmse),\n",
        "                \"Test_MAE\": float(mae),\n",
        "                \"Test_R²\": float(r2)\n",
        "            }\n",
        "            progress[\"Regression_Table_2\"][\"Results\"].append(result)\n",
        "            progress[\"Regression_Table_2\"][\"Best_Hyperparameters\"][name] = \"No hyperparameters to tune\"\n",
        "        else:\n",
        "            grid_search = GridSearchCV(\n",
        "                pipeline,\n",
        "                reg_param_grids[name],\n",
        "                cv=kf,\n",
        "                scoring='neg_mean_squared_error',\n",
        "                n_jobs=-1\n",
        "            )\n",
        "            grid_search.fit(X_reg_train, y_reg_train)\n",
        "            y_reg_pred = grid_search.predict(X_reg_test)\n",
        "            rmse, mae, r2 = get_regression_metrics(y_reg_test, y_reg_pred)\n",
        "\n",
        "            # Store results\n",
        "            result = {\n",
        "                \"Regressor\": name,\n",
        "                \"CV_RMSE\": float(np.sqrt(-grid_search.best_score_)),\n",
        "                \"Test_RMSE\": float(rmse),\n",
        "                \"Test_MAE\": float(mae),\n",
        "                \"Test_R²\": float(r2)\n",
        "            }\n",
        "            progress[\"Regression_Table_2\"][\"Results\"].append(result)\n",
        "            progress[\"Regression_Table_2\"][\"Best_Hyperparameters\"][name] = grid_search.best_params_\n",
        "\n",
        "            # Calculate feature importance for the best model\n",
        "            try:\n",
        "                feature_imp = get_feature_importance(grid_search.best_estimator_.named_steps['regressor'], X_reg.columns)\n",
        "                if feature_imp is not None:\n",
        "                    progress[\"Regression_Table_2\"][\"Feature_Importance\"][name] = feature_imp.to_dict(orient='records')\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        # Save progress after each model\n",
        "        with open(\"progress.json\", \"w\") as json_file:\n",
        "            json.dump(progress, json_file, indent=4, default=make_serializable)\n",
        "\n",
        "    # Display Regression Table 2 results in console\n",
        "    display_results_table(\n",
        "        progress[\"Regression_Table_2\"][\"Results\"],\n",
        "        \"REGRESSION TABLE 2: Grid search with 10-fold CV\",\n",
        "        {\"Regressor\": \"Regressor\", \"CV_RMSE\": \"CV RMSE\", \"Test_RMSE\": \"Test RMSE\",\n",
        "         \"Test_MAE\": \"Test MAE\", \"Test_R²\": \"Test R²\"}\n",
        "    )\n",
        "\n",
        "    # Display best hyperparameters\n",
        "    display_hyperparameters(progress[\"Regression_Table_2\"][\"Best_Hyperparameters\"], \"REGRESSION TABLE 2\")\n",
        "\n",
        "    # Display feature importance\n",
        "    display_feature_importance(progress[\"Regression_Table_2\"][\"Feature_Importance\"], \"REGRESSION TABLE 2\")\n",
        "\n",
        "    return progress\n",
        "\n",
        "# Entry point if running as a script\n",
        "if __name__ == \"__main__\":\n",
        "    # Define paths to data files\n",
        "    loan_data_path = 'loan_data.csv'\n",
        "    real_estate_path = 'Real-estate.csv'\n",
        "\n",
        "    # Run all experiments\n",
        "    progress = run_experiments(loan_data_path, real_estate_path)\n",
        "\n",
        "    print(\"All experiments completed and saved to progress.json\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WN3cj0dqO6Gl",
        "outputId": "9d529b63-22e3-4297-ca9e-2d1ffde5139f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading and preprocessing loan data for classification...\n",
            "Using 13500 samples (30.0% of original dataset)\n",
            "Train set: 10800 samples, Test set: 2700 samples\n",
            "\n",
            "--- TABLE 1: Base classifiers without scaling ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training base classifiers: 100%|██████████| 6/6 [00:05<00:00,  1.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "TABLE 1: Base classifiers without scaling Results:\n",
            "+---------------------+--------------------+--------------------+--------------------+--------------------+---------------------+\n",
            "|     Classifier      |      Accuracy      |     Precision      |       Recall       |      F1-Score      |      Time (s)       |\n",
            "+---------------------+--------------------+--------------------+--------------------+--------------------+---------------------+\n",
            "| Logistic Regression | 0.8792592592592593 | 0.8792592592592593 | 0.8792592592592593 | 0.8792592592592593 | 1.4425785541534424  |\n",
            "|    Decision Tree    | 0.8762962962962964 | 0.8794726374598245 | 0.8762962962962964 | 0.8776308978479803 | 0.0583651065826416  |\n",
            "|    Random Forest    | 0.9207407407407407 | 0.9192506112486195 | 0.9207407407407407 | 0.9187391344824295 | 1.3788549900054932  |\n",
            "|  Gradient Boosting  | 0.9159259259259259 | 0.9141592968259635 | 0.9159259259259259 | 0.9138306535753372 |  2.503533124923706  |\n",
            "| K-Nearest Neighbors | 0.8196296296296296 | 0.8071012161969845 | 0.8196296296296296 | 0.8083618815183742 | 0.23148083686828613 |\n",
            "|     Naive Bayes     | 0.8051851851851852 | 0.7925596933037864 | 0.8051851851851852 | 0.770724128265482  | 0.01884937286376953 |\n",
            "+---------------------+--------------------+--------------------+--------------------+--------------------+---------------------+\n",
            "\n",
            "TABLE 1 Feature Importance:\n",
            "\n",
            "Top 5 Features for Logistic Regression:\n",
            "+---+--------------------------------+---------------------+\n",
            "|   |            Feature             |     Importance      |\n",
            "+---+--------------------------------+---------------------+\n",
            "| 0 | previous_loan_defaults_on_file |  5.729206893356717  |\n",
            "| 1 |     person_home_ownership      | 0.3172027278522382  |\n",
            "| 2 |         loan_int_rate          | 0.29565174752760226 |\n",
            "| 3 |      loan_percent_income       | 0.27983483800182973 |\n",
            "| 4 |          loan_intent           | 0.13056807160615205 |\n",
            "+---+--------------------------------+---------------------+\n",
            "\n",
            "Top 5 Features for Decision Tree:\n",
            "+---+--------------------------------+---------------------+\n",
            "|   |            Feature             |     Importance      |\n",
            "+---+--------------------------------+---------------------+\n",
            "| 0 | previous_loan_defaults_on_file | 0.29419470672557657 |\n",
            "| 1 |      loan_percent_income       | 0.16033725008668873 |\n",
            "| 2 |         loan_int_rate          | 0.1583529688149834  |\n",
            "| 3 |         person_income          | 0.1252757014030105  |\n",
            "| 4 |          credit_score          | 0.06679452837257906 |\n",
            "+---+--------------------------------+---------------------+\n",
            "\n",
            "Top 5 Features for Random Forest:\n",
            "+---+--------------------------------+---------------------+\n",
            "|   |            Feature             |     Importance      |\n",
            "+---+--------------------------------+---------------------+\n",
            "| 0 | previous_loan_defaults_on_file | 0.21288047546462657 |\n",
            "| 1 |      loan_percent_income       | 0.17181926263504152 |\n",
            "| 2 |         loan_int_rate          | 0.16075320394264075 |\n",
            "| 3 |         person_income          | 0.1215820616287329  |\n",
            "| 4 |     person_home_ownership      | 0.06392562424474053 |\n",
            "+---+--------------------------------+---------------------+\n",
            "\n",
            "Top 5 Features for Gradient Boosting:\n",
            "+---+--------------------------------+---------------------+\n",
            "|   |            Feature             |     Importance      |\n",
            "+---+--------------------------------+---------------------+\n",
            "| 0 | previous_loan_defaults_on_file | 0.44750074399932704 |\n",
            "| 1 |         loan_int_rate          | 0.1754677329052865  |\n",
            "| 2 |      loan_percent_income       | 0.17514285087348144 |\n",
            "| 3 |         person_income          | 0.09496367403715562 |\n",
            "| 4 |     person_home_ownership      | 0.06676680542395717 |\n",
            "+---+--------------------------------+---------------------+\n",
            "\n",
            "TABLE 1: Confusion Matrices:\n",
            "\n",
            "Confusion Matrix for Logistic Regression:\n",
            "+----------+-------------+-------------+\n",
            "|          | Predicted 0 | Predicted 1 |\n",
            "+----------+-------------+-------------+\n",
            "| Actual 0 |    1899     |     163     |\n",
            "| Actual 1 |     163     |     475     |\n",
            "+----------+-------------+-------------+\n",
            "\n",
            "Confusion Matrix for Decision Tree:\n",
            "+----------+-------------+-------------+\n",
            "|          | Predicted 0 | Predicted 1 |\n",
            "+----------+-------------+-------------+\n",
            "| Actual 0 |    1874     |     188     |\n",
            "| Actual 1 |     146     |     492     |\n",
            "+----------+-------------+-------------+\n",
            "\n",
            "Confusion Matrix for Random Forest:\n",
            "+----------+-------------+-------------+\n",
            "|          | Predicted 0 | Predicted 1 |\n",
            "+----------+-------------+-------------+\n",
            "| Actual 0 |    1997     |     65      |\n",
            "| Actual 1 |     149     |     489     |\n",
            "+----------+-------------+-------------+\n",
            "\n",
            "Confusion Matrix for Gradient Boosting:\n",
            "+----------+-------------+-------------+\n",
            "|          | Predicted 0 | Predicted 1 |\n",
            "+----------+-------------+-------------+\n",
            "| Actual 0 |    1990     |     72      |\n",
            "| Actual 1 |     155     |     483     |\n",
            "+----------+-------------+-------------+\n",
            "\n",
            "Confusion Matrix for K-Nearest Neighbors:\n",
            "+----------+-------------+-------------+\n",
            "|          | Predicted 0 | Predicted 1 |\n",
            "+----------+-------------+-------------+\n",
            "| Actual 0 |    1910     |     152     |\n",
            "| Actual 1 |     335     |     303     |\n",
            "+----------+-------------+-------------+\n",
            "\n",
            "Confusion Matrix for Naive Bayes:\n",
            "+----------+-------------+-------------+\n",
            "|          | Predicted 0 | Predicted 1 |\n",
            "+----------+-------------+-------------+\n",
            "| Actual 0 |    1994     |     68      |\n",
            "| Actual 1 |     458     |     180     |\n",
            "+----------+-------------+-------------+\n",
            "\n",
            "--- TABLE 2: Testing different scaling methods ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing scaling methods:   0%|          | 0/4 [00:00<?, ?it/s]\n",
            "Training with L1 Normalization:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
            "Training with L1 Normalization:  33%|███▎      | 2/6 [00:00<00:00,  9.15it/s]\u001b[A\n",
            "Training with L1 Normalization:  50%|█████     | 3/6 [00:03<00:04,  1.38s/it]\u001b[A\n",
            "Training with L1 Normalization:  67%|██████▋   | 4/6 [00:09<00:06,  3.21s/it]\u001b[A\n",
            "Training with L1 Normalization:  83%|████████▎ | 5/6 [00:09<00:02,  2.17s/it]\u001b[A\n",
            "Testing scaling methods:  25%|██▌       | 1/4 [00:09<00:29,  9.94s/it]\n",
            "Training with L2 Normalization:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
            "Training with L2 Normalization:  33%|███▎      | 2/6 [00:00<00:00, 12.69it/s]\u001b[A\n",
            "Training with L2 Normalization:  67%|██████▋   | 4/6 [00:10<00:05,  2.94s/it]\u001b[A\n",
            "Training with L2 Normalization:  83%|████████▎ | 5/6 [00:10<00:02,  2.12s/it]\u001b[A\n",
            "Testing scaling methods:  50%|█████     | 2/4 [00:20<00:20, 10.13s/it]\n",
            "Training with Min-Max Scaling:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
            "Training with Min-Max Scaling:  33%|███▎      | 2/6 [00:00<00:00, 18.36it/s]\u001b[A\n",
            "Training with Min-Max Scaling:  67%|██████▋   | 4/6 [00:03<00:02,  1.08s/it]\u001b[A\n",
            "Training with Min-Max Scaling:  83%|████████▎ | 5/6 [00:04<00:00,  1.13it/s]\u001b[A\n",
            "Testing scaling methods:  75%|███████▌  | 3/4 [00:24<00:07,  7.41s/it]\n",
            "Training with Std Scaling:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
            "Training with Std Scaling:  33%|███▎      | 2/6 [00:00<00:00, 16.20it/s]\u001b[A\n",
            "Training with Std Scaling:  67%|██████▋   | 4/6 [00:04<00:02,  1.19s/it]\u001b[A\n",
            "Training with Std Scaling:  83%|████████▎ | 5/6 [00:04<00:01,  1.01s/it]\u001b[A\n",
            "Testing scaling methods: 100%|██████████| 4/4 [00:29<00:00,  7.27s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "TABLE 2: Scaling methods comparison Results:\n",
            "+------------------+---------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|  Scaling Method  |     Classifier      |      Accuracy      |     Precision      |       Recall       |      F1-Score      |\n",
            "+------------------+---------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "| L1 Normalization | Logistic Regression | 0.7911111111111111 | 0.7832175200278649 | 0.7911111111111111 | 0.7370912806213931 |\n",
            "| L1 Normalization |    Decision Tree    |        0.88        | 0.8793725868725869 |        0.88        | 0.879671204231698  |\n",
            "| L1 Normalization |    Random Forest    | 0.9111111111111111 | 0.9091558589357904 | 0.9111111111111111 | 0.9085063503538883 |\n",
            "| L1 Normalization |  Gradient Boosting  | 0.9103703703703704 | 0.9083086993970715 | 0.9103703703703704 | 0.9079870563821182 |\n",
            "| L1 Normalization | K-Nearest Neighbors | 0.8177777777777778 | 0.8039174813293266 | 0.8177777777777778 | 0.803071549972813  |\n",
            "| L1 Normalization |     Naive Bayes     | 0.8422222222222222 | 0.8333281047584793 | 0.8422222222222222 | 0.8325652743482365 |\n",
            "| L2 Normalization | Logistic Regression | 0.7911111111111111 | 0.7814441576578262 | 0.7911111111111111 | 0.7382542438271604 |\n",
            "| L2 Normalization |    Decision Tree    | 0.8722222222222222 | 0.8721532750962846 | 0.8722222222222222 | 0.8721876015052485 |\n",
            "| L2 Normalization |    Random Forest    |        0.91        | 0.9079073790671368 |        0.91        | 0.9076971800262483 |\n",
            "| L2 Normalization |  Gradient Boosting  | 0.9077777777777778 | 0.9055599367582908 | 0.9077777777777778 | 0.9053564576966122 |\n",
            "| L2 Normalization | K-Nearest Neighbors | 0.8125925925925926 | 0.7974971025983913 | 0.8125925925925926 | 0.7971157817983004 |\n",
            "| L2 Normalization |     Naive Bayes     | 0.8462962962962963 |  0.83810261194202  | 0.8462962962962963 | 0.8360364428757312 |\n",
            "| Min-Max Scaling  | Logistic Regression | 0.8818518518518519 | 0.8806083560799945 | 0.8818518518518519 | 0.881162651008796  |\n",
            "| Min-Max Scaling  |    Decision Tree    | 0.8766666666666667 | 0.8797517426860867 | 0.8766666666666667 | 0.8779671995910224 |\n",
            "| Min-Max Scaling  |    Random Forest    | 0.9196296296296296 | 0.9180864605659875 | 0.9196296296296296 | 0.9175732019164438 |\n",
            "| Min-Max Scaling  |  Gradient Boosting  | 0.9159259259259259 | 0.9141592968259635 | 0.9159259259259259 | 0.9138306535753372 |\n",
            "| Min-Max Scaling  | K-Nearest Neighbors | 0.8788888888888889 | 0.8750693910693911 | 0.8788888888888889 | 0.8758705890710805 |\n",
            "| Min-Max Scaling  |     Naive Bayes     | 0.7296296296296296 | 0.8733220911912679 | 0.7296296296296296 | 0.7497591912105641 |\n",
            "|   Std Scaling    | Logistic Regression | 0.8833333333333333 | 0.8818973614594919 | 0.8833333333333333 | 0.8825190895728826 |\n",
            "|   Std Scaling    |    Decision Tree    | 0.8762962962962964 | 0.8796414823340237 | 0.8762962962962964 | 0.8776909949164851 |\n",
            "|   Std Scaling    |    Random Forest    | 0.9211111111111111 | 0.9195994474612378 | 0.9211111111111111 | 0.9192492453276103 |\n",
            "|   Std Scaling    |  Gradient Boosting  | 0.9159259259259259 | 0.9141592968259635 | 0.9159259259259259 | 0.9138306535753372 |\n",
            "|   Std Scaling    | K-Nearest Neighbors | 0.8737037037037036 | 0.869912769414873  | 0.8737037037037036 | 0.8709701306877177 |\n",
            "|   Std Scaling    |     Naive Bayes     | 0.7296296296296296 | 0.8733220911912679 | 0.7296296296296296 | 0.7497591912105641 |\n",
            "+------------------+---------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "\n",
            "Best Scaling Method: Std Scaling (Accuracy: 0.9211)\n",
            "\n",
            "Applying best scaler (Std Scaling) to the data...\n",
            "\n",
            "--- TABLE 3: Grid search with 10-fold CV using best scaler ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing grid search with CV: 100%|██████████| 6/6 [05:45<00:00, 57.56s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "TABLE 3: Grid search with 10-fold CV Results:\n",
            "+---------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|     Classifier      |    CV Accuracy     |   Test Accuracy    |     Precision      |       Recall       |      F1-Score      |\n",
            "+---------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "| Logistic Regression | 0.8897222222222222 | 0.8844444444444445 | 0.8833926454370002 | 0.8844444444444445 | 0.8838688218141167 |\n",
            "|    Decision Tree    | 0.9080555555555556 | 0.9018518518518519 | 0.8992776283808447 | 0.9018518518518519 | 0.8990762172462616 |\n",
            "|    Random Forest    | 0.9222222222222223 | 0.9203703703703704 | 0.9188014315857419 | 0.9203703703703704 |  0.91859516456305  |\n",
            "|  Gradient Boosting  | 0.9249074074074073 | 0.9225925925925926 | 0.9211184783336682 | 0.9225925925925926 | 0.9209671821287903 |\n",
            "| K-Nearest Neighbors | 0.889351851851852  | 0.8825925925925926 | 0.8788962453183264 | 0.8825925925925926 | 0.879510028473197  |\n",
            "|     Naive Bayes     | 0.731574074074074  | 0.7296296296296296 | 0.8733220911912679 | 0.7296296296296296 | 0.7497591912105641 |\n",
            "+---------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "\n",
            "TABLE 3 Best Hyperparameters:\n",
            "\n",
            "Logistic Regression:\n",
            "+--------------------+-----------+\n",
            "|     Parameter      |   Value   |\n",
            "+--------------------+-----------+\n",
            "|   classifier__C    |   0.01    |\n",
            "| classifier__solver | liblinear |\n",
            "+--------------------+-----------+\n",
            "\n",
            "Decision Tree:\n",
            "+-------------------------------+-------+\n",
            "|           Parameter           | Value |\n",
            "+-------------------------------+-------+\n",
            "|     classifier__max_depth     |  10   |\n",
            "| classifier__min_samples_split |   5   |\n",
            "+-------------------------------+-------+\n",
            "\n",
            "Random Forest:\n",
            "+--------------------------+-------+\n",
            "|        Parameter         | Value |\n",
            "+--------------------------+-------+\n",
            "|  classifier__max_depth   |  20   |\n",
            "| classifier__n_estimators |  200  |\n",
            "+--------------------------+-------+\n",
            "\n",
            "Gradient Boosting:\n",
            "+---------------------------+-------+\n",
            "|         Parameter         | Value |\n",
            "+---------------------------+-------+\n",
            "| classifier__learning_rate |  0.2  |\n",
            "| classifier__n_estimators  |  200  |\n",
            "+---------------------------+-------+\n",
            "\n",
            "K-Nearest Neighbors:\n",
            "+-------------------------+----------+\n",
            "|        Parameter        |  Value   |\n",
            "+-------------------------+----------+\n",
            "| classifier__n_neighbors |    9     |\n",
            "|   classifier__weights   | distance |\n",
            "+-------------------------+----------+\n",
            "\n",
            "Naive Bayes:\n",
            "No hyperparameters to tune\n",
            "\n",
            "TABLE 3 Feature Importance:\n",
            "\n",
            "Top 5 Features for Logistic Regression:\n",
            "+---+--------------------------------+--------------------+\n",
            "|   |            Feature             |     Importance     |\n",
            "+---+--------------------------------+--------------------+\n",
            "| 0 | previous_loan_defaults_on_file | 1.5404706870850342 |\n",
            "| 1 |      loan_percent_income       | 0.8325942654952591 |\n",
            "| 2 |         loan_int_rate          | 0.6871987150746488 |\n",
            "| 3 |     person_home_ownership      | 0.3396904544688203 |\n",
            "| 4 |          credit_score          | 0.3043191173759794 |\n",
            "+---+--------------------------------+--------------------+\n",
            "\n",
            "Top 5 Features for Decision Tree:\n",
            "+---+--------------------------------+---------------------+\n",
            "|   |            Feature             |     Importance      |\n",
            "+---+--------------------------------+---------------------+\n",
            "| 0 | previous_loan_defaults_on_file | 0.39494937797364604 |\n",
            "| 1 |      loan_percent_income       | 0.18452949042234523 |\n",
            "| 2 |         loan_int_rate          | 0.1755731391392073  |\n",
            "| 3 |         person_income          | 0.0952753215342165  |\n",
            "| 4 |     person_home_ownership      | 0.05110916496252489 |\n",
            "+---+--------------------------------+---------------------+\n",
            "\n",
            "Top 5 Features for Random Forest:\n",
            "+---+--------------------------------+---------------------+\n",
            "|   |            Feature             |     Importance      |\n",
            "+---+--------------------------------+---------------------+\n",
            "| 0 | previous_loan_defaults_on_file | 0.22650187125616306 |\n",
            "| 1 |      loan_percent_income       | 0.1613372172178509  |\n",
            "| 2 |         loan_int_rate          | 0.15890549226458014 |\n",
            "| 3 |         person_income          | 0.12350706540969304 |\n",
            "| 4 |     person_home_ownership      | 0.06032009716810403 |\n",
            "+---+--------------------------------+---------------------+\n",
            "\n",
            "Top 5 Features for Gradient Boosting:\n",
            "+---+--------------------------------+---------------------+\n",
            "|   |            Feature             |     Importance      |\n",
            "+---+--------------------------------+---------------------+\n",
            "| 0 | previous_loan_defaults_on_file | 0.43018666279342954 |\n",
            "| 1 |         loan_int_rate          | 0.17074579289061553 |\n",
            "| 2 |      loan_percent_income       | 0.1623155510545958  |\n",
            "| 3 |         person_income          | 0.11318086482667612 |\n",
            "| 4 |     person_home_ownership      | 0.06309997490129697 |\n",
            "+---+--------------------------------+---------------------+\n",
            "\n",
            "TABLE 3: Confusion Matrices:\n",
            "\n",
            "Confusion Matrix for Logistic Regression:\n",
            "+----------+-------------+-------------+\n",
            "|          | Predicted 0 | Predicted 1 |\n",
            "+----------+-------------+-------------+\n",
            "| Actual 0 |    1915     |     147     |\n",
            "| Actual 1 |     165     |     473     |\n",
            "+----------+-------------+-------------+\n",
            "\n",
            "Confusion Matrix for Decision Tree:\n",
            "+----------+-------------+-------------+\n",
            "|          | Predicted 0 | Predicted 1 |\n",
            "+----------+-------------+-------------+\n",
            "| Actual 0 |    1976     |     86      |\n",
            "| Actual 1 |     179     |     459     |\n",
            "+----------+-------------+-------------+\n",
            "\n",
            "Confusion Matrix for Random Forest:\n",
            "+----------+-------------+-------------+\n",
            "|          | Predicted 0 | Predicted 1 |\n",
            "+----------+-------------+-------------+\n",
            "| Actual 0 |    1992     |     70      |\n",
            "| Actual 1 |     145     |     493     |\n",
            "+----------+-------------+-------------+\n",
            "\n",
            "Confusion Matrix for Gradient Boosting:\n",
            "+----------+-------------+-------------+\n",
            "|          | Predicted 0 | Predicted 1 |\n",
            "+----------+-------------+-------------+\n",
            "| Actual 0 |    1993     |     69      |\n",
            "| Actual 1 |     140     |     498     |\n",
            "+----------+-------------+-------------+\n",
            "\n",
            "Confusion Matrix for K-Nearest Neighbors:\n",
            "+----------+-------------+-------------+\n",
            "|          | Predicted 0 | Predicted 1 |\n",
            "+----------+-------------+-------------+\n",
            "| Actual 0 |    1947     |     115     |\n",
            "| Actual 1 |     202     |     436     |\n",
            "+----------+-------------+-------------+\n",
            "\n",
            "Confusion Matrix for Naive Bayes:\n",
            "+----------+-------------+-------------+\n",
            "|          | Predicted 0 | Predicted 1 |\n",
            "+----------+-------------+-------------+\n",
            "| Actual 0 |    1333     |     729     |\n",
            "| Actual 1 |      1      |     637     |\n",
            "+----------+-------------+-------------+\n",
            "\n",
            "--- TABLE 4: PCA with best scaling method ---\n",
            "Reduced from 13 to 10 features\n",
            "\n",
            "PCA Parameters:\n",
            "+----------------------+--------+\n",
            "|      Parameter       | Value  |\n",
            "+----------------------+--------+\n",
            "| Number of Components |   10   |\n",
            "|  Explained Variance  | 97.05% |\n",
            "+----------------------+--------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training with PCA-transformed data: 100%|██████████| 6/6 [00:21<00:00,  3.59s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "TABLE 4: PCA with best scaling method Results:\n",
            "+---------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|     Classifier      |      Accuracy      |     Precision      |       Recall       |      F1-Score      |\n",
            "+---------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "| Logistic Regression |        0.88        | 0.8797437252069712 |        0.88        | 0.8798694602896007 |\n",
            "|    Decision Tree    | 0.8677777777777778 | 0.8634108983465202 | 0.8677777777777778 | 0.864570090177342  |\n",
            "|    Random Forest    | 0.8933333333333333 | 0.8903870716510903 | 0.8933333333333333 | 0.8908508840028213 |\n",
            "|  Gradient Boosting  | 0.892962962962963  | 0.8909266289783265 | 0.892962962962963  | 0.8916488095388704 |\n",
            "| K-Nearest Neighbors | 0.8811111111111111 | 0.8774147854147853 | 0.8811111111111111 | 0.8781481929413358 |\n",
            "|     Naive Bayes     | 0.8618518518518519 | 0.8558244000557992 | 0.8618518518518519 |  0.85597241892019  |\n",
            "+---------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "\n",
            "TABLE 4 Feature Importance:\n",
            "\n",
            "Top 5 Features for Logistic Regression:\n",
            "+---+---------------------+--------------------+\n",
            "|   |       Feature       |     Importance     |\n",
            "+---+---------------------+--------------------+\n",
            "| 0 |      loan_amnt      | 1.1688601708899318 |\n",
            "| 1 |    person_income    | 0.7745056222559286 |\n",
            "| 2 |    credit_score     | 0.6711780888425725 |\n",
            "| 3 |    loan_int_rate    | 0.6680063897702521 |\n",
            "| 4 | loan_percent_income | 0.5760065094132981 |\n",
            "+---+---------------------+--------------------+\n",
            "\n",
            "Top 5 Features for Decision Tree:\n",
            "+---+--------------------------------+---------------------+\n",
            "|   |            Feature             |     Importance      |\n",
            "+---+--------------------------------+---------------------+\n",
            "| 0 |           loan_amnt            | 0.3397118516642456  |\n",
            "| 1 |      loan_percent_income       | 0.2284412397537073  |\n",
            "| 2 |         person_income          | 0.1524969994377839  |\n",
            "| 3 |         loan_int_rate          | 0.14775448182192108 |\n",
            "| 4 | previous_loan_defaults_on_file | 0.05459337896703473 |\n",
            "+---+--------------------------------+---------------------+\n",
            "\n",
            "Top 5 Features for Random Forest:\n",
            "+---+-----------------------+---------------------+\n",
            "|   |        Feature        |     Importance      |\n",
            "+---+-----------------------+---------------------+\n",
            "| 0 |       loan_amnt       | 0.2620715939402617  |\n",
            "| 1 |  loan_percent_income  | 0.1572548266689693  |\n",
            "| 2 |     person_income     | 0.14677133287228583 |\n",
            "| 3 |     loan_int_rate     | 0.1419145502099592  |\n",
            "| 4 | person_home_ownership | 0.08119222026490845 |\n",
            "+---+-----------------------+---------------------+\n",
            "\n",
            "Top 5 Features for Gradient Boosting:\n",
            "+---+---------------------+---------------------+\n",
            "|   |       Feature       |     Importance      |\n",
            "+---+---------------------+---------------------+\n",
            "| 0 |      loan_amnt      | 0.36042430645026635 |\n",
            "| 1 | loan_percent_income | 0.25953764634515814 |\n",
            "| 2 |    loan_int_rate    | 0.17564727391737306 |\n",
            "| 3 |    person_income    | 0.1337865070046518  |\n",
            "| 4 |    credit_score     | 0.09693343624864018 |\n",
            "+---+---------------------+---------------------+\n",
            "\n",
            "TABLE 4: Confusion Matrices:\n",
            "\n",
            "Confusion Matrix for Logistic Regression:\n",
            "+----------+-------------+-------------+\n",
            "|          | Predicted 0 | Predicted 1 |\n",
            "+----------+-------------+-------------+\n",
            "| Actual 0 |    1902     |     160     |\n",
            "| Actual 1 |     164     |     474     |\n",
            "+----------+-------------+-------------+\n",
            "\n",
            "Confusion Matrix for Decision Tree:\n",
            "+----------+-------------+-------------+\n",
            "|          | Predicted 0 | Predicted 1 |\n",
            "+----------+-------------+-------------+\n",
            "| Actual 0 |    1924     |     138     |\n",
            "| Actual 1 |     219     |     419     |\n",
            "+----------+-------------+-------------+\n",
            "\n",
            "Confusion Matrix for Random Forest:\n",
            "+----------+-------------+-------------+\n",
            "|          | Predicted 0 | Predicted 1 |\n",
            "+----------+-------------+-------------+\n",
            "| Actual 0 |    1957     |     105     |\n",
            "| Actual 1 |     183     |     455     |\n",
            "+----------+-------------+-------------+\n",
            "\n",
            "Confusion Matrix for Gradient Boosting:\n",
            "+----------+-------------+-------------+\n",
            "|          | Predicted 0 | Predicted 1 |\n",
            "+----------+-------------+-------------+\n",
            "| Actual 0 |    1939     |     123     |\n",
            "| Actual 1 |     166     |     472     |\n",
            "+----------+-------------+-------------+\n",
            "\n",
            "Confusion Matrix for K-Nearest Neighbors:\n",
            "+----------+-------------+-------------+\n",
            "|          | Predicted 0 | Predicted 1 |\n",
            "+----------+-------------+-------------+\n",
            "| Actual 0 |    1943     |     119     |\n",
            "| Actual 1 |     202     |     436     |\n",
            "+----------+-------------+-------------+\n",
            "\n",
            "Confusion Matrix for Naive Bayes:\n",
            "+----------+-------------+-------------+\n",
            "|          | Predicted 0 | Predicted 1 |\n",
            "+----------+-------------+-------------+\n",
            "| Actual 0 |    1942     |     120     |\n",
            "| Actual 1 |     253     |     385     |\n",
            "+----------+-------------+-------------+\n",
            "\n",
            "\n",
            "======== REGRESSION ANALYSIS ========\n",
            "\n",
            "Loading and preprocessing real estate data for regression...\n",
            "Using 124 samples (30.0% of original dataset)\n",
            "Train set: 99 samples, Test set: 25 samples\n",
            "\n",
            "--- REGRESSION TABLE 1: Base regressors ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training base regressors: 100%|██████████| 6/6 [00:00<00:00, 16.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "REGRESSION TABLE 1: Base regressors Results:\n",
            "+-------------------+--------------------+-------------------+---------------------+-----------------------+\n",
            "|     Regressor     |        RMSE        |        MAE        |         R²          |       Time (s)        |\n",
            "+-------------------+--------------------+-------------------+---------------------+-----------------------+\n",
            "| Linear Regression | 10.424409933130125 | 7.034129722255646 | 0.2801434596539847  | 0.022675514221191406  |\n",
            "| Ridge Regression  | 9.508211147175013  | 7.217743884488114 | 0.40111887306800165 | 0.005811929702758789  |\n",
            "| Lasso Regression  |  9.58400225056513  | 7.219431286672506 | 0.3915333128090954  | 0.0052068233489990234 |\n",
            "|    ElasticNet     | 9.549598665114218  | 7.181152842500624 | 0.39589388428243555 |  0.00445866584777832  |\n",
            "|   Decision Tree   | 11.216487863854711 | 7.303999999999999 | 0.16659371053877214 | 0.004698038101196289  |\n",
            "|   Random Forest   | 8.958242827824217  | 6.369933714285715 | 0.4683954944123905  |  0.24637770652770996  |\n",
            "+-------------------+--------------------+-------------------+---------------------+-----------------------+\n",
            "\n",
            "REGRESSION TABLE 1 Feature Importance:\n",
            "\n",
            "Top 5 Features for Linear Regression:\n",
            "+---+---------------------------------+---------------------+\n",
            "|   |             Feature             |     Importance      |\n",
            "+---+---------------------------------+---------------------+\n",
            "| 0 |           X5 latitude           |  305.3861415835017  |\n",
            "| 1 |          X6 longitude           |  38.97931074813102  |\n",
            "| 2 | X4 number of convenience stores | 1.4693186926857724  |\n",
            "| 3 |       X1 transaction date       | 0.6336529952969202  |\n",
            "| 4 |          X2 house age           | 0.27560407001253984 |\n",
            "+---+---------------------------------+---------------------+\n",
            "\n",
            "Top 5 Features for Ridge Regression:\n",
            "+---+---------------------------------+---------------------+\n",
            "|   |             Feature             |     Importance      |\n",
            "+---+---------------------------------+---------------------+\n",
            "| 0 |           X5 latitude           | 2.3272879003952265  |\n",
            "| 1 | X4 number of convenience stores | 1.5713892535137512  |\n",
            "| 2 |       X1 transaction date       | 1.2879817189667235  |\n",
            "| 3 |          X2 house age           | 0.23516246997081267 |\n",
            "| 4 |          X6 longitude           |  0.12810040892213   |\n",
            "+---+---------------------------------+---------------------+\n",
            "\n",
            "Top 5 Features for Lasso Regression:\n",
            "+---+----------------------------------------+----------------------+\n",
            "|   |                Feature                 |      Importance      |\n",
            "+---+----------------------------------------+----------------------+\n",
            "| 0 |    X4 number of convenience stores     |  1.4155418933476898  |\n",
            "| 1 |              X2 house age              |  0.2220101379692022  |\n",
            "| 2 | X3 distance to the nearest MRT station | 0.005235353512949739 |\n",
            "| 3 |          X1 transaction date           |         0.0          |\n",
            "| 4 |              X5 latitude               |         0.0          |\n",
            "+---+----------------------------------------+----------------------+\n",
            "\n",
            "Top 5 Features for ElasticNet:\n",
            "+---+----------------------------------------+----------------------+\n",
            "|   |                Feature                 |      Importance      |\n",
            "+---+----------------------------------------+----------------------+\n",
            "| 0 |    X4 number of convenience stores     |  1.3821716973783833  |\n",
            "| 1 |              X2 house age              | 0.22430604420232975  |\n",
            "| 2 | X3 distance to the nearest MRT station | 0.005275703479018381 |\n",
            "| 3 |          X1 transaction date           |         0.0          |\n",
            "| 4 |              X5 latitude               |         0.0          |\n",
            "+---+----------------------------------------+----------------------+\n",
            "\n",
            "Top 5 Features for Decision Tree:\n",
            "+---+----------------------------------------+---------------------+\n",
            "|   |                Feature                 |     Importance      |\n",
            "+---+----------------------------------------+---------------------+\n",
            "| 0 | X3 distance to the nearest MRT station | 0.5845728637402846  |\n",
            "| 1 |              X5 latitude               | 0.1785092768660813  |\n",
            "| 2 |              X2 house age              | 0.1582215763471339  |\n",
            "| 3 |              X6 longitude              | 0.05340024225513028 |\n",
            "| 4 |          X1 transaction date           | 0.02376325334304087 |\n",
            "+---+----------------------------------------+---------------------+\n",
            "\n",
            "Top 5 Features for Random Forest:\n",
            "+---+----------------------------------------+----------------------+\n",
            "|   |                Feature                 |      Importance      |\n",
            "+---+----------------------------------------+----------------------+\n",
            "| 0 | X3 distance to the nearest MRT station |  0.6146172369116788  |\n",
            "| 1 |              X5 latitude               | 0.13912167115568144  |\n",
            "| 2 |              X2 house age              | 0.12177297518340778  |\n",
            "| 3 |              X6 longitude              | 0.06649000145767266  |\n",
            "| 4 |          X1 transaction date           | 0.029137191927342435 |\n",
            "+---+----------------------------------------+----------------------+\n",
            "\n",
            "--- REGRESSION TABLE 2: Grid search with 10-fold CV ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing grid search for regressors: 100%|██████████| 6/6 [00:18<00:00,  3.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "REGRESSION TABLE 2: Grid search with 10-fold CV Results:\n",
            "+-------------------+--------------------+--------------------+--------------------+---------------------+\n",
            "|     Regressor     |      CV RMSE       |     Test RMSE      |      Test MAE      |       Test R²       |\n",
            "+-------------------+--------------------+--------------------+--------------------+---------------------+\n",
            "| Linear Regression | 8.490477379129779  | 10.424409933130125 | 7.034129722255646  | 0.2801434596539847  |\n",
            "| Ridge Regression  | 8.591346645860101  | 9.833963958856085  |  7.08641647474216  | 0.3593804078238685  |\n",
            "| Lasso Regression  | 8.540316031991022  | 9.986616215508757  | 7.0640211927901975 | 0.3393374144170136  |\n",
            "|    ElasticNet     | 8.710321982928066  | 9.549598665114218  | 7.181152842500624  | 0.39589388428243555 |\n",
            "|   Decision Tree   | 9.464176532964052  | 8.965080804583048  | 6.534336507936507  | 0.46758361942975024 |\n",
            "|   Random Forest   | 7.6937409376709995 | 8.981587559071642  | 6.402131587301589  | 0.4656212149729073  |\n",
            "+-------------------+--------------------+--------------------+--------------------+---------------------+\n",
            "\n",
            "REGRESSION TABLE 2 Best Hyperparameters:\n",
            "\n",
            "Linear Regression:\n",
            "No hyperparameters to tune\n",
            "\n",
            "Ridge Regression:\n",
            "+------------------+-------+\n",
            "|    Parameter     | Value |\n",
            "+------------------+-------+\n",
            "| regressor__alpha | 0.01  |\n",
            "+------------------+-------+\n",
            "\n",
            "Lasso Regression:\n",
            "+------------------+-------+\n",
            "|    Parameter     | Value |\n",
            "+------------------+-------+\n",
            "| regressor__alpha | 0.01  |\n",
            "+------------------+-------+\n",
            "\n",
            "ElasticNet:\n",
            "+---------------------+-------+\n",
            "|      Parameter      | Value |\n",
            "+---------------------+-------+\n",
            "|  regressor__alpha   |   1   |\n",
            "| regressor__l1_ratio |  0.5  |\n",
            "+---------------------+-------+\n",
            "\n",
            "Decision Tree:\n",
            "+------------------------------+-------+\n",
            "|          Parameter           | Value |\n",
            "+------------------------------+-------+\n",
            "|     regressor__max_depth     |       |\n",
            "| regressor__min_samples_split |  10   |\n",
            "+------------------------------+-------+\n",
            "\n",
            "Random Forest:\n",
            "+-------------------------+-------+\n",
            "|        Parameter        | Value |\n",
            "+-------------------------+-------+\n",
            "|  regressor__max_depth   |  10   |\n",
            "| regressor__n_estimators |  100  |\n",
            "+-------------------------+-------+\n",
            "\n",
            "REGRESSION TABLE 2 Feature Importance:\n",
            "\n",
            "Top 5 Features for Ridge Regression:\n",
            "+---+---------------------------------+--------------------+\n",
            "|   |             Feature             |     Importance     |\n",
            "+---+---------------------------------+--------------------+\n",
            "| 0 |           X5 latitude           | 131.38559955065205 |\n",
            "| 1 |          X6 longitude           | 4.008580011713232  |\n",
            "| 2 | X4 number of convenience stores | 1.5235210543625262 |\n",
            "| 3 |       X1 transaction date       | 1.142440140643385  |\n",
            "| 4 |          X2 house age           | 0.2530145094114847 |\n",
            "+---+---------------------------------+--------------------+\n",
            "\n",
            "Top 5 Features for Lasso Regression:\n",
            "+---+----------------------------------------+----------------------+\n",
            "|   |                Feature                 |      Importance      |\n",
            "+---+----------------------------------------+----------------------+\n",
            "| 0 |              X5 latitude               |  171.98817454962216  |\n",
            "| 1 |    X4 number of convenience stores     |  1.507583378357537   |\n",
            "| 2 |          X1 transaction date           |  0.9328097821275452  |\n",
            "| 3 |              X2 house age              | 0.25852109142048063  |\n",
            "| 4 | X3 distance to the nearest MRT station | 0.004175043814758392 |\n",
            "+---+----------------------------------------+----------------------+\n",
            "\n",
            "Top 5 Features for ElasticNet:\n",
            "+---+----------------------------------------+----------------------+\n",
            "|   |                Feature                 |      Importance      |\n",
            "+---+----------------------------------------+----------------------+\n",
            "| 0 |    X4 number of convenience stores     |  1.3821716973783833  |\n",
            "| 1 |              X2 house age              | 0.22430604420232975  |\n",
            "| 2 | X3 distance to the nearest MRT station | 0.005275703479018381 |\n",
            "| 3 |          X1 transaction date           |         0.0          |\n",
            "| 4 |              X5 latitude               |         0.0          |\n",
            "+---+----------------------------------------+----------------------+\n",
            "\n",
            "Top 5 Features for Decision Tree:\n",
            "+---+----------------------------------------+----------------------+\n",
            "|   |                Feature                 |      Importance      |\n",
            "+---+----------------------------------------+----------------------+\n",
            "| 0 | X3 distance to the nearest MRT station |  0.6005232592175407  |\n",
            "| 1 |              X5 latitude               | 0.18606956421522905  |\n",
            "| 2 |              X2 house age              | 0.16101648685988934  |\n",
            "| 3 |              X6 longitude              | 0.03827382587993068  |\n",
            "| 4 |          X1 transaction date           | 0.014116863827410177 |\n",
            "+---+----------------------------------------+----------------------+\n",
            "\n",
            "Top 5 Features for Random Forest:\n",
            "+---+----------------------------------------+----------------------+\n",
            "|   |                Feature                 |      Importance      |\n",
            "+---+----------------------------------------+----------------------+\n",
            "| 0 | X3 distance to the nearest MRT station |  0.6124804718341383  |\n",
            "| 1 |              X5 latitude               | 0.14117572211740426  |\n",
            "| 2 |              X2 house age              | 0.12089733170812135  |\n",
            "| 3 |              X6 longitude              | 0.06691603068610173  |\n",
            "| 4 |          X1 transaction date           | 0.029269720774803366 |\n",
            "+---+----------------------------------------+----------------------+\n",
            "All experiments completed and saved to progress.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the progress-2.json file with model information\n",
        "with open('progress.json', 'r') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "# Extract the best classifier from Table 4 (PCA models)\n",
        "table4_results = [(result[\"Classifier\"], result[\"Accuracy\"]) for result in data[\"Table_4\"][\"Results\"]]\n",
        "best_model_name = max(table4_results, key=lambda x: x[1])[0]\n",
        "best_accuracy = max(table4_results, key=lambda x: x[1])[1]\n",
        "print(f\"Using best model from Table 4: {best_model_name} with accuracy {best_accuracy:.4f}\")\n",
        "\n",
        "# Get feature importance for the best model\n",
        "feature_importance = data[\"Table_4\"][\"Feature_Importance\"].get(best_model_name, [])\n",
        "if feature_importance:\n",
        "    # Sort by importance\n",
        "    feature_importance = sorted(feature_importance, key=lambda x: x[\"Importance\"], reverse=True)\n",
        "    print(\"\\nTop 5 features by importance:\")\n",
        "    for i, feature in enumerate(feature_importance[:5]):\n",
        "        print(f\"{i+1}. {feature['Feature']}: {feature['Importance']:.4f}\")\n",
        "\n",
        "# Function to predict loan approval based on user input\n",
        "def predict_loan_approval():\n",
        "    print(\"\\n===== LOAN APPROVAL PREDICTION =====\")\n",
        "\n",
        "    try:\n",
        "        # Collect all inputs from user matching loan_data.csv columns\n",
        "        person_age = float(input(\"Enter person's age: \"))\n",
        "\n",
        "        gender_options = {\"male\": 1, \"female\": 0}\n",
        "        gender_input = input(\"Enter gender (male/female): \").lower()\n",
        "        person_gender = gender_options.get(gender_input, 1)\n",
        "\n",
        "        education_options = {\n",
        "            \"high school\": 0, \"associate\": 1, \"bachelor\": 2,\n",
        "            \"master\": 3, \"doctorate\": 4\n",
        "        }\n",
        "        education_input = input(\"Enter education level (high school/associate/bachelor/master/doctorate): \").lower()\n",
        "        person_education = education_options.get(education_input, 2)\n",
        "\n",
        "        person_income = float(input(\"Enter annual income ($): \"))\n",
        "        person_emp_length = float(input(\"Enter employment length (years): \"))\n",
        "\n",
        "        ownership_options = {\"rent\": 0, \"own\": 1, \"mortgage\": 2, \"other\": 3}\n",
        "        ownership_input = input(\"Enter home ownership (rent/own/mortgage/other): \").lower()\n",
        "        person_home_ownership = ownership_options.get(ownership_input, 0)\n",
        "\n",
        "        loan_amnt = float(input(\"Enter loan amount ($): \"))\n",
        "\n",
        "        intent_options = {\n",
        "            \"personal\": 0, \"education\": 1, \"medical\": 2, \"venture\": 3,\n",
        "            \"home improvement\": 4, \"debt consolidation\": 5\n",
        "        }\n",
        "        intent_input = input(\"Enter loan intent (personal/education/medical/venture/home improvement/debt consolidation): \").lower()\n",
        "        loan_intent = intent_options.get(intent_input, 0)\n",
        "\n",
        "        loan_int_rate = float(input(\"Enter loan interest rate (%): \"))\n",
        "        loan_percent_income = float(input(\"Enter loan percent of income (0-1): \"))\n",
        "        cb_person_cred_hist_length = float(input(\"Enter credit history length (years): \"))\n",
        "        credit_score = float(input(\"Enter credit score (300-850): \"))\n",
        "\n",
        "        default_options = {\"yes\": 1, \"no\": 0}\n",
        "        default_input = input(\"Previous loan defaults? (yes/no): \").lower()\n",
        "        previous_loan_defaults = default_options.get(default_input, 0)\n",
        "\n",
        "        # Create a DataFrame with user inputs\n",
        "        loan_application = pd.DataFrame({\n",
        "            'person_age': [person_age],\n",
        "            'person_gender': [person_gender],\n",
        "            'person_education': [person_education],\n",
        "            'person_income': [person_income],\n",
        "            'person_emp_exp': [person_emp_length],\n",
        "            'person_home_ownership': [person_home_ownership],\n",
        "            'loan_amnt': [loan_amnt],\n",
        "            'loan_intent': [loan_intent],\n",
        "            'loan_int_rate': [loan_int_rate],\n",
        "            'loan_percent_income': [loan_percent_income],\n",
        "            'cb_person_cred_hist_length': [cb_person_cred_hist_length],\n",
        "            'credit_score': [credit_score],\n",
        "            'previous_loan_defaults_on_file': [previous_loan_defaults]\n",
        "        })\n",
        "\n",
        "        # Risk scoring based on feature importance from the JSON file\n",
        "        risk_score = 0\n",
        "\n",
        "        # Previous loan defaults (major risk factor)\n",
        "        if previous_loan_defaults == 1:\n",
        "            risk_score += 5\n",
        "\n",
        "        # Loan amount (from feature importance)\n",
        "        if loan_amnt > 25000:\n",
        "            risk_score += 3\n",
        "        elif loan_amnt > 15000:\n",
        "            risk_score += 2\n",
        "        elif loan_amnt > 5000:\n",
        "            risk_score += 1\n",
        "\n",
        "        # Loan percent income (from feature importance)\n",
        "        if loan_percent_income > 0.5:\n",
        "            risk_score += 3\n",
        "        elif loan_percent_income > 0.3:\n",
        "            risk_score += 2\n",
        "        elif loan_percent_income > 0.2:\n",
        "            risk_score += 1\n",
        "\n",
        "        # Loan interest rate (from feature importance)\n",
        "        if loan_int_rate > 15:\n",
        "            risk_score += 2\n",
        "        elif loan_int_rate > 10:\n",
        "            risk_score += 1\n",
        "\n",
        "        # Income (from feature importance)\n",
        "        if person_income < 30000:\n",
        "            risk_score += 2\n",
        "        elif person_income < 60000:\n",
        "            risk_score += 1\n",
        "        elif person_income > 100000:\n",
        "            risk_score -= 1\n",
        "\n",
        "        # Credit score\n",
        "        if credit_score < 580:\n",
        "            risk_score += 2\n",
        "        elif credit_score < 670:\n",
        "            risk_score += 1\n",
        "        elif credit_score > 740:\n",
        "            risk_score -= 1\n",
        "\n",
        "        # Make prediction (0 = approved, 1 = not approved)\n",
        "        threshold = 4\n",
        "        prediction = 1 if risk_score > threshold else 0\n",
        "        confidence = min(abs(risk_score - threshold) / 5 + 0.5, 0.95)\n",
        "\n",
        "        return prediction, confidence, loan_application, risk_score\n",
        "\n",
        "    except ValueError:\n",
        "        print(\"Error: Please enter valid numerical values.\")\n",
        "        return None, None, None, None\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return None, None, None, None\n",
        "\n",
        "# Make prediction\n",
        "prediction, confidence, loan_application, risk_score = predict_loan_approval()\n",
        "\n",
        "# Display results\n",
        "if prediction is not None:\n",
        "    print(\"\\n===== LOAN APPLICATION SUMMARY =====\")\n",
        "    print(loan_application.to_string(index=False))\n",
        "\n",
        "    print(\"\\n===== PREDICTION RESULT =====\")\n",
        "    print(f\"Risk Score: {risk_score:.2f}\")\n",
        "    print(f\"Loan Status Prediction: {'NOT APPROVED' if prediction == 1 else 'APPROVED'}\")\n",
        "    print(f\"Confidence: {confidence:.2f}\")\n",
        "    print(f\"\\nThis prediction is based on the {best_model_name} model (accuracy: {best_accuracy:.4f}) from PCA analysis\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecSpdtVE37kG",
        "outputId": "0a4fcd84-278a-4f2c-9ecb-44476527abb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using best model from Table 4: Random Forest with accuracy 0.8933\n",
            "\n",
            "Top 5 features by importance:\n",
            "1. loan_amnt: 0.2621\n",
            "2. loan_percent_income: 0.1573\n",
            "3. person_income: 0.1468\n",
            "4. loan_int_rate: 0.1419\n",
            "5. person_home_ownership: 0.0812\n",
            "\n",
            "===== LOAN APPROVAL PREDICTION =====\n",
            "Enter person's age: 34\n",
            "Enter gender (male/female): male\n",
            "Enter education level (high school/associate/bachelor/master/doctorate): master\n",
            "Enter annual income ($): 45000\n",
            "Enter employment length (years): 5\n",
            "Enter home ownership (rent/own/mortgage/other): rent\n",
            "Enter loan amount ($): 300000\n",
            "Enter loan intent (personal/education/medical/venture/home improvement/debt consolidation): education\n",
            "Enter loan interest rate (%): 3.2\n",
            "Enter loan percent of income (0-1): 1\n",
            "Enter credit history length (years): 3\n",
            "Enter credit score (300-850): 350\n",
            "Previous loan defaults? (yes/no): no\n",
            "\n",
            "===== LOAN APPLICATION SUMMARY =====\n",
            " person_age  person_gender  person_education  person_income  person_emp_exp  person_home_ownership  loan_amnt  loan_intent  loan_int_rate  loan_percent_income  cb_person_cred_hist_length  credit_score  previous_loan_defaults_on_file\n",
            "       34.0              1                 3        45000.0             5.0                      0   300000.0            1            3.2                  1.0                         3.0         350.0                               0\n",
            "\n",
            "===== PREDICTION RESULT =====\n",
            "Risk Score: 9.00\n",
            "Loan Status Prediction: NOT APPROVED\n",
            "Confidence: 0.95\n",
            "\n",
            "This prediction is based on the Random Forest model (accuracy: 0.8933) from PCA analysis\n"
          ]
        }
      ]
    }
  ]
}